// Simple audio-envelope cross-correlation analyzer to find time offset between two video elements.
// Notes:
// - Called from a user gesture (button click) so autoplay/audio capture should be allowed.
// - Uses short-time RMS envelope (frameWindowMs) then normalized cross-correlation to find lag.
// - Returns offset = (timeB - timeA) in seconds (positive => B lags A).
// - Lightweight and robust for music-sync detection (not sample-perfect).

export async function computeEnvelopeFromVideoElement(
  videoEl,
  frameWindowMs = 50,
  maxDurationMs = null
) {
  const tempVideo = videoEl.cloneNode(true);
  tempVideo.muted = true; // prevent duplicate sound
  tempVideo.currentTime = videoEl.currentTime;

  return new Promise(async (resolve, reject) => {
    try {
      const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
      const source = audioCtx.createMediaElementSource(tempVideo);
      const analyser = audioCtx.createAnalyser();
      analyser.fftSize = 2048;
      source.connect(analyser);
      analyser.connect(audioCtx.destination);

      const buf = new Float32Array(analyser.fftSize);
      const envelope = [];
      const sampleInterval = frameWindowMs;
      const durationMs =
        maxDurationMs ??
        (isFinite(videoEl.duration) ? videoEl.duration * 1000 : 0);

      // ensure start at 0
      videoEl.currentTime = 0;

      // user gesture should allow this play()
      await videoEl.play().catch((e) => {
        // if play rejected, still try to continue but fail gracefully
        console.warn("video play rejected while capturing audio envelope", e);
      });

      let elapsed = 0;
      const intervalId = setInterval(() => {
        try {
          analyser.getFloatTimeDomainData(buf);
          // compute RMS of buffer
          let sum = 0;
          for (let i = 0; i < buf.length; i++) sum += buf[i] * buf[i];
          const rms = Math.sqrt(sum / buf.length);
          envelope.push(rms);
        } catch (err) {
          console.warn("analyser read error", err);
        }
        elapsed += sampleInterval;
        if ((durationMs > 0 && elapsed >= durationMs) || videoEl.ended) {
          clearInterval(intervalId);
          videoEl.pause();
          source.disconnect();
          analyser.disconnect();
          audioCtx.close();
          resolve({ envelope, frameRate: 1000 / frameWindowMs });
        }
      }, sampleInterval);
    } catch (err) {
      reject(err);
    }
  });
}

function normalizedCrossCorrelation(a, b, maxLagFrames) {
  // returns {lagFrames, score}
  const Na = a.length,
    Nb = b.length;
  let best = { lag: 0, score: -Infinity };
  // precompute sumsquares for normalization windows on-the-fly
  for (let lag = -maxLagFrames; lag <= maxLagFrames; lag++) {
    let num = 0,
      sumA2 = 0,
      sumB2 = 0,
      count = 0;
    for (let i = 0; i < Na; i++) {
      const j = i + lag;
      if (j < 0 || j >= Nb) continue;
      const va = a[i],
        vb = b[j];
      num += va * vb;
      sumA2 += va * va;
      sumB2 += vb * vb;
      count++;
    }
    if (count === 0) continue;
    const denom = Math.sqrt(sumA2 * sumB2);
    const score = denom > 0 ? num / denom : 0;
    if (score > best.score) best = { lag, score };
  }
  return best;
}

/**
 * Compute time offset between two video elements (B relative to A).
 * Options: frameWindowMs (envelope window) and maxLagSec (search range).
 * Returns offsetSeconds (timeB - timeA): positive => B starts later.
 */
export async function computeOffsetBetweenVideos(videoA, videoB, opts = {}) {
  const frameWindowMs = opts.frameWindowMs ?? 50;
  const maxLagSec = opts.maxLagSec ?? 10;
  const maxDurationMs = Math.min(
    isFinite(videoA.duration) ? videoA.duration * 1000 : Infinity,
    isFinite(videoB.duration) ? videoB.duration * 1000 : Infinity
  );

  // ensure both videos are at time=0
  videoA.currentTime = 0;
  videoB.currentTime = 0;

  // Capture envelopes in parallel. Both will be played (user gesture).
  const [rA, rB] = await Promise.all([
    computeEnvelopeFromVideoElement(videoA, frameWindowMs, maxDurationMs),
    computeEnvelopeFromVideoElement(videoB, frameWindowMs, maxDurationMs),
  ]);

  const envA = rA.envelope;
  const envB = rB.envelope;
  const frameRate = rA.frameRate;

  const maxLagFrames = Math.floor(maxLagSec * frameRate);

  const best = normalizedCrossCorrelation(envA, envB, maxLagFrames);
  const offsetSeconds = best.lag / frameRate; // positive => B lags A

  return { offsetSeconds, score: best.score, frameRate, envA, envB };
}
